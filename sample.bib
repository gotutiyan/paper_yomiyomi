@inproceedings{dummy_all_null,
}

@inproceedings{dahlmeier-ng-2012-better,
    title = "Better Evaluation for Grammatical Error Correction",
    author = "Dahlmeier, Daniel  and
      Ng, Hwee Tou",
    booktitle = "Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N12-1067",
    pages = "568--572",
}

@inproceedings{bryant-etal-2017-automatic,
    title = "Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction",
    author = "Bryant, Christopher  and
      Felice, Mariano  and
      Briscoe, Ted",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1074",
    doi = "10.18653/v1/P17-1074",
    pages = "793--805",
    abstract = "Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rule-based framework. This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets. Human experts rated the automatic edits as {``}Good{''} or {``}Acceptable{''} in at least 95{\%} of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time.",
}

@inproceedings{choshen-abend-2018-reference,
    title = "Reference-less Measure of Faithfulness for Grammatical Error Correction",
    author = "Choshen, Leshem  and
      Abend, Omri",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-2020",
    doi = "10.18653/v1/N18-2020",
    pages = "124--129",
    abstract = "We propose USim, a semantic measure for Grammatical Error Correction (that measures the semantic faithfulness of the output to the source, thereby complementing existing reference-less measures (RLMs) for measuring the output{'}s grammaticality. USim operates by comparing the semantic symbolic structure of the source and the correction, without relying on manually-curated references. Our experiments establish the validity of USim, by showing that the semantic structures can be consistently applied to ungrammatical text, that valid corrections obtain a high USim similarity score to the source, and that invalid corrections obtain a lower score.",
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{mutton-etal-2007-gleu,
    title = "{GLEU}: Automatic Evaluation of Sentence-Level Fluency",
    author = "Mutton, Andrew  and
      Dras, Mark  and
      Wan, Stephen  and
      Dale, Robert",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P07-1044",
    pages = "344--351",
}

@inproceedings{mita-etal-2019-cross,
    title = "Cross-Corpora Evaluation and Analysis of Grammatical Error Correction Models {---} Is Single-Corpus Evaluation Enough?",
    author = "Mita, Masato  and
      Mizumoto, Tomoya  and
      Kaneko, Masahiro  and
      Nagata, Ryo  and
      Inui, Kentaro",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1132",
    doi = "10.18653/v1/N19-1132",
    pages = "1309--1314",
    abstract = "This study explores the necessity of performing cross-corpora evaluation for grammatical error correction (GEC) models. GEC models have been previously evaluated based on a single commonly applied corpus: the CoNLL-2014 benchmark. However, the evaluation remains incomplete because the task difficulty varies depending on the test corpus and conditions such as the proficiency levels of the writers and essay topics. To overcome this limitation, we evaluate the performance of several GEC models, including NMT-based (LSTM, CNN, and transformer) and an SMT-based model, against various learner corpora (CoNLL-2013, CoNLL-2014, FCE, JFLEG, ICNALE, and KJ). Evaluation results reveal that the models{'} rankings considerably vary depending on the corpus, indicating that single-corpus evaluation is insufficient for GEC models.",
}

@inproceedings{grundkiewicz-etal-2019-neural,
    title = "Neural Grammatical Error Correction Systems with Unsupervised Pre-training on Synthetic Data",
    author = "Grundkiewicz, Roman  and
      Junczys-Dowmunt, Marcin  and
      Heafield, Kenneth",
    booktitle = "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4427",
    doi = "10.18653/v1/W19-4427",
    pages = "252--263",
    abstract = "Considerable effort has been made to address the data sparsity problem in neural grammatical error correction. In this work, we propose a simple and surprisingly effective unsupervised synthetic error generation method based on confusion sets extracted from a spellchecker to increase the amount of training data. Synthetic data is used to pre-train a Transformer sequence-to-sequence model, which not only improves over a strong baseline trained on authentic error-annotated data, but also enables the development of a practical GEC system in a scenario where little genuine error-annotated data is available. The developed systems placed first in the BEA19 shared task, achieving 69.47 and 64.24 F$_{0.5}$ in the restricted and low-resource tracks respectively, both on the W{\&}I+LOCNESS test set. On the popular CoNLL 2014 test set, we report state-of-the-art results of 64.16 M{\mbox{$^2$}} for the submitted system, and 61.30 M{\mbox{$^2$}} for the constrained system trained on the NUCLE and Lang-8 data.",
}

@inproceedings{sennrich-etal-2016-improving,
    title = "Improving Neural Machine Translation Models with Monolingual Data",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1009",
    doi = "10.18653/v1/P16-1009",
    pages = "86--96",
}

@inproceedings{xie-etal-2018-noising,
    title = "Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction",
    author = "Xie, Ziang  and
      Genthial, Guillaume  and
      Xie, Stanley  and
      Ng, Andrew  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1057",
    doi = "10.18653/v1/N18-1057",
    pages = "619--628",
    abstract = "Translation-based methods for grammar correction that directly map noisy, ungrammatical text to their clean counterparts are able to correct a broad range of errors; however, such techniques are bottlenecked by the need for a large parallel corpus of noisy and clean sentence pairs. In this paper, we consider synthesizing parallel data by noising a clean monolingual corpus. While most previous approaches introduce perturbations using features computed from local context windows, we instead develop error generation processes using a neural sequence transduction model trained to translate clean examples to their noisy counterparts. Given a corpus of clean examples, we propose beam search noising procedures to synthesize additional noisy examples that human evaluators were nearly unable to discriminate from nonsynthesized examples. Surprisingly, when trained on additional data synthesized using our best-performing noising scheme, our model approaches the same performance as when trained on additional nonsynthesized data.",
}

@inproceedings{zhao-etal-2019-improving,
    title = "Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data",
    author = "Zhao, Wei  and
      Wang, Liang  and
      Shen, Kewei  and
      Jia, Ruoyu  and
      Liu, Jingming",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1014",
    doi = "10.18653/v1/N19-1014",
    pages = "156--165",
    abstract = "Neural machine translation systems have become state-of-the-art approaches for Grammatical Error Correction (GEC) task. In this paper, we propose a copy-augmented architecture for the GEC task by copying the unchanged words from the source sentence to the target sentence. Since the GEC suffers from not having enough labeled training data to achieve high accuracy. We pre-train the copy-augmented architecture with a denoising auto-encoder using the unlabeled One Billion Benchmark and make comparisons between the fully pre-trained model and a partially pre-trained model. It is the first time copying words from the source context and fully pre-training a sequence to sequence model are experimented on the GEC task. Moreover, We add token-level and sentence-level multi-task learning for the GEC task. The evaluation results on the CoNLL-2014 test set show that our approach outperforms all recently published state-of-the-art results by a large margin.",
}


@inproceedings{kiyono-etal-2019-empirical,
    title = "An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction",
    author = "Kiyono, Shun  and
      Suzuki, Jun  and
      Mita, Masato  and
      Mizumoto, Tomoya  and
      Inui, Kentaro",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1119",
    doi = "10.18653/v1/D19-1119",
    pages = "1236--1242",
    abstract = "The incorporation of pseudo data in the training of grammatical error correction models has been one of the main factors in improving the performance of such models. However, consensus is lacking on experimental configurations, namely, choosing how the pseudo data should be generated or used. In this study, these choices are investigated through extensive experiments, and state-of-the-art performance is achieved on the CoNLL-2014 test set (F0.5=65.0) and the official test set of the BEA-2019 shared task (F0.5=70.2) without making any modifications to the model architecture.",
}

@inproceedings{kiyono:nlp:2020,
  title="大規模疑似データを用いた高性能文法誤り訂正モデルの構築",
  author="清野 舜 and 鈴木 潤 and 三田 雅人 and 水本 智也 and 乾 健太郎",
  booktitle="言語処理学会 第26回年次大会",
  year="2020",
  url="https://www.anlp.jp/proceedings/annual_meeting/2020/pdf_dir/D4-1.pdf"
}

@inproceedings{ng-etal-2013-conll,
    title = "The {C}o{NLL}-2013 Shared Task on Grammatical Error Correction",
    author = "Ng, Hwee Tou  and
      Wu, Siew Mei  and
      Wu, Yuanbin  and
      Hadiwinoto, Christian  and
      Tetreault, Joel",
    booktitle = "Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W13-3601",
    pages = "1--12",
}

@inproceedings{ng-etal-2014-conll,
    title = "The {C}o{NLL}-2014 Shared Task on Grammatical Error Correction",
    author = "Ng, Hwee Tou  and
      Wu, Siew Mei  and
      Briscoe, Ted  and
      Hadiwinoto, Christian  and
      Susanto, Raymond Hendy  and
      Bryant, Christopher",
    booktitle = "Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-1701",
    doi = "10.3115/v1/W14-1701",
    pages = "1--14",
}

@inproceedings{bryant-etal-2019-bea,
    title = "The {BEA}-2019 Shared Task on Grammatical Error Correction",
    author = "Bryant, Christopher  and
      Felice, Mariano  and
      Andersen, {\O}istein E.  and
      Briscoe, Ted",
    booktitle = "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4406",
    doi = "10.18653/v1/W19-4406",
    pages = "52--75",
    abstract = "This paper reports on the BEA-2019 Shared Task on Grammatical Error Correction (GEC). As with the CoNLL-2014 shared task, participants are required to correct all types of errors in test data. One of the main contributions of the BEA-2019 shared task is the introduction of a new dataset, the Write{\&}Improve+LOCNESS corpus, which represents a wider range of native and learner English levels and abilities. Another contribution is the introduction of tracks, which control the amount of annotated data available to participants. Systems are evaluated in terms of ERRANT F{\_}0.5, which allows us to report a much wider range of performance statistics. The competition was hosted on Codalab and remains open for further submissions on the blind test set.",
}

@inproceedings{bryant-ng-2015-far,
    title = "How Far are We from Fully Automatic High Quality Grammatical Error Correction?",
    author = "Bryant, Christopher  and
      Ng, Hwee Tou",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-1068",
    doi = "10.3115/v1/P15-1068",
    pages = "697--707",
}

@inproceedings{kudo-2018-subword,
    title = "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates",
    author = "Kudo, Taku",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1007",
    doi = "10.18653/v1/P18-1007",
    pages = "66--75",
    abstract = "Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.",
}

@inproceedings{kudo-richardson-2018-sentencepiece,
    title = "{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    author = "Kudo, Taku  and
      Richardson, John",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-2012",
    doi = "10.18653/v1/D18-2012",
    pages = "66--71",
    abstract = "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at \url{https://github.com/google/sentencepiece}.",
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press},
  url={http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  url={https://arxiv.org/abs/1706.03762}
  year={2017}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{otani-etal-2016-irt,
    title = "{IRT}-based Aggregation Model of Crowdsourced Pairwise Comparison for Evaluating Machine Translations",
    author = "Otani, Naoki  and
      Nakazawa, Toshiaki  and
      Kawahara, Daisuke  and
      Kurohashi, Sadao",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1049",
    doi = "10.18653/v1/D16-1049",
    pages = "511--520",
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009},
  url="https://dl.acm.org/doi/abs/10.1145/1553374.1553380"
}

@article{ishikawa2013icnale,
  title={The ICNALE and sophisticated contrastive interlanguage analysis of Asian learners of English},
  author={Ishikawa, Shin’ichiro},
  journal={Learner corpus studies in Asia and the world},
  volume={1},
  pages={91--118},
  year={2013},
  publisher={神戸大学国際コミュニケーションセンター},
  url = {http://www.lib.kobe-u.ac.jp/infolib/meta_pub/G0000003kernel_81006678}
}

@inproceedings{napoles-etal-2017-jfleg,
    title = "{JFLEG}: A Fluency Corpus and Benchmark for Grammatical Error Correction",
    author = "Napoles, Courtney  and
      Sakaguchi, Keisuke  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-2037",
    pages = "229--234",
    abstract = "We present a new parallel corpus, JHU FLuency-Extended GUG corpus (JFLEG) for developing and evaluating grammatical error correction (GEC). Unlike other corpora, it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct grammatical errors but also make the original text more native sounding. We describe the types of corrections made and benchmark four leading GEC systems on this corpus, identifying specific areas in which they do well and how they can improve. JFLEG fulfills the need for a new gold standard to properly assess the current state of GEC.",
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013},
  url={https://arxiv.org/abs/1301.3781}
}

@article{永田亮2005英語名詞の可算,
  title={英語名詞の可算/不可算判定手法},
  author={永田亮 and 桝井文人 and 河合敦夫 and 井須尚紀},
  journal={自然言語処理},
  volume={12},
  number={4},
  pages={227--243},
  year={2005},
  publisher={一般社団法人 言語処理学会},
  url = "https://www.jstage.jst.go.jp/article/jnlp1994/12/4/12_4_227/_article/-char/ja/"
}

@inproceedings{五藤:nlp:2020,
  title="訂正難易度を考慮した文法誤り訂正のための性能評価尺度",
  author="五藤巧 and 永田亮 and 三田雅人 and 塙一晃",
  booktitle="言語処理学会 第26回年次大会",
  year="2020",
  url="https://www.anlp.jp/proceedings/annual_meeting/2020/pdf_dir/P2-3.pdf"
}

@inproceedings{三田:nlp:2020,
  title="文法誤り訂正のための自己改良戦略に基づくノイズ除去",
  author="三田雅人 and 清野舜 and 金子正弘 and 鈴木潤 and 乾健太郎",
  booktitle="言語処理学会 第26回年次大会",
  year="2020",
  url="https://www.anlp.jp/proceedings/annual_meeting/2020/pdf_dir/D4-2.pdf"
}

